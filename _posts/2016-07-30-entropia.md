---
layout: post_with_math
title: "Entropía"
---

# Entropía

<br>

La entropía mide la incertidumbre de una fuente de información. A cuanto mayor
sea esta incertidumbre, mayor será este valor. Está estrechamente relacionada
con el segundo principio de la termodinámica:
*La cantidad de entropía del universo tiende a incrementarse en el tiempo.*

Si partimos desde el Big Bang, en este estado tendremos el mayor orden posible,
donde la entropía sera mínima. Mientras que a medida que transcurra el tiempo y
el universo se expanda el desorden irá en aumento, aumentando así
su entropía.

Un ejemplo más sencillo para entender este concepto es midiendo la entropía al
lanzar una moneda al aire. Como no sabemos si saldrá cara o cruz, ambos estados
son equiprobables, con lo que la entropía será máxima. En cambio, si por algún
defecto en la moneda hay un 75% de probabilidades de que salga cara, la entropía
se verá entonces reducida.

Veamos como calcular estos valores.

El valor de entropía se define mediante la siguiente formula (la base del
logaritmo puede cambiar según el caso):

$$H(X) = \sum _{i}p(x_{i})\log _{2}p(\frac{1}{x_{i}})$$

Donde $$p(x)$$ es la probabilidad de que ocurra dicho evento. En nuestro primer caso
tendríamos:

$$
p(cara) = 0.5 \\
p(cruz) = 0.5 \\

H(X) = 0.5\log _{2}(2) + 0.5\log _{2}(2) = 1
$$

Mientras que en el segundo caso:

$$
p(cara) = 0.75 \\
p(cruz) = 0.25 \\

H(X) = 0.75\log _{2}(1.33) + 0.25\log _{2}(4) \approx 0.80
$$

Vemos como se ha reducido en este caso la entropía.

<br>

#### Referencias

- https://es.wikipedia.org/wiki/Entrop%C3%ADa_(informaci%C3%B3n)
- https://es.wikipedia.org/wiki/Segundo_principio_de_la_termodinámica
